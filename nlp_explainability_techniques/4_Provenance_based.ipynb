{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Provenance-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provenance-based technique is that in which explanations are provided by illustrating some or all of the prediction derivation process. This process is intuitive and effective and the final prediction is the result of a series of reasoning steps.\n",
    "\n",
    "Danilevsky et al. proposed the following papers back then:\n",
    "* [Interpretable Relevant Emotion Ranking with Event-Driven Attention](https://aclanthology.org/D19-1017.pdf)\n",
    "    * They mention a model called IRER-EA but we couldn't test it since we couldn't find it online.\n",
    "\n",
    "* [MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms](https://aclanthology.org/N19-1245/).\n",
    "    * This project consists on the creation of a dataset called [MathQA](https://huggingface.co/datasets/math_qa) that includes the chain of thought to solve a mathematical problem. \n",
    "    * The motivation behind it was to provide explainability the Google's AQuA dataset (a dataset with mathematical problems and four options to choose).\n",
    "\n",
    "In 2024, this kind of definition is quite stablished in what we call \"Chain of Thought\", so we could include any model that applies this in this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments - MathQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will simply show how the structure of the AQuA dataset is and how the new dataset, MathQA turned out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaribalta/Documents/EHU/DeepLearning/FinalProject/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_qa = load_dataset(\"math_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = math_qa[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Problem': \"the banker ' s gain of a certain sum due 3 years hence at 10 % \"\n",
      "            'per annum is rs . 36 . what is the present worth ?',\n",
      " 'Rationale': '\"explanation : t = 3 years r = 10 % td = ( bg × 100 ) / tr = ( '\n",
      "              '36 × 100 ) / ( 3 × 10 ) = 12 × 10 = rs . 120 td = ( pw × tr ) / '\n",
      "              '100 ⇒ 120 = ( pw × 3 × 10 ) / 100 ⇒ 1200 = pw × 3 pw = 1200 / 3 '\n",
      "              '= rs . 400 answer : option a\"',\n",
      " 'annotated_formula': 'divide(multiply(const_100, divide(multiply(36, '\n",
      "                      'const_100), multiply(3, 10))), multiply(3, 10))',\n",
      " 'category': 'gain',\n",
      " 'correct': 'a',\n",
      " 'linear_formula': 'multiply(n2,const_100)|multiply(n0,n1)|divide(#0,#1)|multiply(#2,const_100)|divide(#3,#1)|',\n",
      " 'options': 'a ) rs . 400 , b ) rs . 300 , c ) rs . 500 , d ) rs . 350 , e ) '\n",
      "            'none of these'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `Problem`, `Rationale`, `options` and `correct` are the original problems stated in **AQuA**.\n",
    "* `Problem`: a mathematical problem explained in plain words.\n",
    "* `Rationale`: the explanation of how to reach the solution.\n",
    "* `Options`: 5 possible results of the problem, only one is correct.\n",
    "* `correct`: the option that is correct.\n",
    "\n",
    "The new **MathQA** dataset includes:\n",
    "* `category`: a category stating what type of problem it is.\n",
    "* `annotated_formula`: the steps to solve the problem in a python-like code. The operations are always represented in the same way with a fixed vocabulary (e.g. divide, multiply, const_X, etc)\n",
    "* `linear_formula`: a formula that states the steps in a clarer way that the previous one. Steps are separated by `|`.\n",
    "\n",
    "In MathQA, the authors use this new version of the dataset and pose the problem as a machine translation problem, with an encoder-decoder aarchitecture. This facilitates the understanding of the problem solving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments - Toy RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, provenance-based techniques explain or show part of the prediction derivation problem. Nowadays, RAG systems could also fall into this category, since part of the final decision relies on the extra information the model retrieves.\n",
    "We will implement a toy example of a RAG system, this is inspired by [A beginner’s guide to building a Retrieval Augmented Generation (RAG) application from scratch](https://medium.com/@wachambers/a-beginners-guide-to-building-a-retrieval-augmented-generation-rag-application-from-scratch-e52921953a5d) from Medium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG stands for Retrieval Agumented Generation and is basically a model that researches and contextualizes with additional information we have provided it with. RAG’s internal knowledge can be easily altered or even supplemented on the fly, enabling researchers and engineers to control what RAG knows and doesn’t know without wasting time or compute power retraining the entire model.\n",
    "In other words, we can state that the essence of RAG involves adding your own data (via a retrieval tool) to the prompt that you pass into a large language model. So, basically, you are altering the input of the LLM and provide extra information, hence, additional explanatinability and more context to get to a certain context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what will be our task in this case? \n",
    "* Since RAG is for generative models, we won't be using our fine-tuned model for classification, nor a BERT. We will be using [Llama2](https://llama.meta.com/).\n",
    "* We will see how a generative model modifies the response when we add the real context behind some verses of our test set.\n",
    "* As we already know, our test set is the song [All too well (10 Minute Version)(Taylor's Version)(From the Vault)](https://www.youtube.com/watch?v=sRxrwjOtIag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our toy-RAG work?\n",
    "\n",
    "The high-level components of a RAG system are a **corpus**, an **input** from the user and **a similarity measure** between the corpus and the user input.\n",
    "\n",
    "In our case, we defined:\n",
    "* The input: verses of the test data, lyrics from the song. The same as in the previous experiments.\n",
    "* The corpus: the corpus works as the ``additional information\" that helps contextualise and enrich the query before reaching the LLM. In this case, we took the comments of the song from the \\href{https://genius.com/Taylor-swift-all-too-well-10-minute-version-taylors-version-from-the-vault-lyrics}{Genius webpage}. Genius provides context and explanations of the lyrics from songs.\n",
    "* The similarity measure is the Jaccard similarity score.\n",
    "\n",
    "\n",
    "The process followed by the RAG system:\n",
    "\n",
    "1. Receive a user input (the lyrics)\n",
    "2. Perform a similarity measure (we chose the Jaccard similarity as in the article) to match the most suitable document (the context from Genius)\n",
    "3. Send to the LLM the query with the added information. Our LLM is Llama2 and we call it via the `ollama` proxy.\n",
    "\\end{enumerate}\n",
    "\n",
    "\n",
    "The Jaccard similarity used for querying the corpus given the input lyric is defined as: \n",
    "\n",
    "$J(i,d) = \\dfrac{|i \\cap d|}{|i \\cup u|}, {i\\in I, d\\in D}$\n",
    "\n",
    "Where:\n",
    "* $D$ is the set of documents, in our case the additional information extracted from Genius.\n",
    "* $I$ is the user input, in our case, the verse from the test.\n",
    "* The intersection $\\cap$ represents the total number of words that appear in both: the verse and the document.\n",
    "* The union $\\cup$ represents the total number of words that appear in the verse or the document.\n",
    "\n",
    "\n",
    "We take the document $d$ that has the higher Jaccard Similarity given the input $i$.\n",
    "\n",
    "The prompt used as input for queries with contextualized information: \n",
    "\n",
    "```You are a classifier that, given a sentence, says if the sentence is negative, positive, neutral or mixed. The sentence is {song_lyric}\". Additional info is that: {additional_information}\". Say which class is more suitable and a short explanation```\n",
    "\n",
    "The prompt used as input for queries without contextualized information:\n",
    "\n",
    "```You are a classifier that, given a sentence, says if the sentence is negative, positive, neutral or mixed. The sentence is {user_input}\". Say which class is more suitable and a short explanation```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess import get_train_dev_test_data\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import requests\n",
    "import json\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Corpus, extracted from the Genius lyric comments of the song:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_of_documents = [\n",
    "    \"'Walking through a door' can be used as a metaphor to indicate the start of something new like a love story\",\n",
    "    \"Leaving it in a drawer signify it's no longer a part of his life\",\n",
    "    \"'Sweet disposition' may refer to the subject's kind, thoughtful personality\",\n",
    "    \"Songs about seeming to be fine, even if feeling shattered inside\",\n",
    "    \"'I was there' could be an indicator that she was being gaslit by her ex partner\",\n",
    "    \"Fans believe this song is about Jake Gyllenhaal, who indeed had glasses when he was younger\",\n",
    "    \"Patriarchy is a sociological term coined by feminist theorists. It describes the system in our society that creates a power imbalance\",\n",
    "    \"It’s possible that he tried to win her back by saying he loved her\",\n",
    "    \"'Three months in the grave' could reference the time after a breakup or a lull in their romance\",\n",
    "    \"keep it like a secret implies Swift’s ex may have wanted to keep their relationship hidden\",\n",
    "    \"'In the name of being honest' can be seen as Swift describing her partner as manipulative\",\n",
    "    \"'All is well that ends well' is an expression that means struggles and difficulty will pass by as long as the outcome is positive.\"\n",
    "    \"To double-cross someone is to deceive or betray them\",\n",
    "    \"the alleged subject of this song, was 9 years older than Taylor Swift when they dated\",\n",
    "    \"The scarf symbolizes his longing and passion for the relationship\",\n",
    "    \"Swift claims that this relationship was the only real one that her ex had\",\n",
    "    \"'sticks and stones may break my bones' mean that it’s clear he put her through some sort of verbal abuse\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our similarity measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    query = query.lower().split(\" \")\n",
    "    document = document.lower().split(\" \")\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that will match our input with the additional information that better matches it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_response(query, corpus):\n",
    "    similarities = []\n",
    "    for doc in corpus:\n",
    "        similarity = jaccard_similarity(query, doc)\n",
    "        similarities.append(similarity)\n",
    "    return corpus_of_documents[similarities.index(max(similarities))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play with our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "_, _, test = get_train_dev_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load examples\n",
    "drawer_verse = test[\"verse_text\"][3]\n",
    "drawer_label = test[\"labels\"][3]\n",
    "secret_verse = test[\"verse_text\"][42]\n",
    "secret_label = test[\"labels\"][42]\n",
    "love_verse = test[\"verse_text\"][25]\n",
    "love_label = test[\"labels\"][25]\n",
    "honest_verse = test[\"verse_text\"][51]\n",
    "honest_label = test[\"labels\"][51]\n",
    "cross_verse = test[\"verse_text\"][56]\n",
    "cross_label = test[\"labels\"][56]\n",
    "brooklyn_verse = test[\"verse_text\"][93]\n",
    "brooklyn_label = test[\"labels\"][93]\n",
    "\n",
    "\n",
    "user_inputs = [\n",
    "    drawer_verse,\n",
    "    secret_verse,\n",
    "    love_verse,\n",
    "    honest_verse,\n",
    "    cross_verse,\n",
    "    brooklyn_verse,\n",
    "]\n",
    "labels = [\n",
    "    drawer_label,\n",
    "    secret_label,\n",
    "    love_label,\n",
    "    honest_label,\n",
    "    cross_label,\n",
    "    brooklyn_label,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input = And you've still got it in your drawer, even now\n",
      "Similarity Response = Leaving it in a drawer signify it's no longer a part of his life\n",
      "\n",
      "User input = You kept me like a secret, but I kept you like an oath\n",
      "Similarity Response = keep it like a secret implies Swift’s ex may have wanted to keep their relationship hidden\n",
      "\n",
      "User input = He's gonna say it's love\n",
      "Similarity Response = Leaving it in a drawer signify it's no longer a part of his life\n",
      "\n",
      "User input = So casually cruel in the name of bein' honest\n",
      "Similarity Response = 'In the name of being honest' can be seen as Swift describing her partner as manipulative\n",
      "\n",
      "User input = You double-cross my mind\n",
      "Similarity Response = 'sticks and stones may break my bones' mean that it’s clear he put her through some sort of verbal abuse\n",
      "\n",
      "User input = From when your Brooklyn broke my skin and bones\n",
      "Similarity Response = 'sticks and stones may break my bones' mean that it’s clear he put her through some sort of verbal abuse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# visualize examples\n",
    "input_and_response = []\n",
    "\n",
    "for user_input in user_inputs:\n",
    "    response = return_response(user_input, corpus_of_documents)\n",
    "    input_and_response.append((user_input, response))\n",
    "    print(f\"User input = {user_input}\")\n",
    "    print(f\"Similarity Response = {return_response(user_input, corpus_of_documents)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that not every input has matched a similar sentence or one that better contextualizes it:\n",
    "* User input = You double-cross my mind\n",
    "* Similarity Response = 'sticks and stones may break my bones' mean that it’s clear he put her through some sort of verbal abuse\n",
    "\n",
    "However, some of them were contextualized:\n",
    "* User input = So casually cruel in the name of bein' honest\n",
    "* Similarity Response = 'In the name of being honest' can be seen as Swift describing her partner as manipulative\n",
    "\n",
    "Now let's generate the prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompts sent to the LLM\n",
    "def create_prompt(user_input, additional_info=None):\n",
    "    if additional_info:\n",
    "        return f\"\"\"You are a classifier that, given a sentence, says if the sentence is negative, positive, neutral or mixed. The sentence is \"{user_input}\". Additional info is that: {additional_info}\". Say which class is more suitable and a short explanation\"\"\"\n",
    "    return f\"\"\"You are a classifier that, given a sentence, says if the sentence is negative, positive, neutral or mixed. The sentence is \"{user_input}\". Say which class is more suitable and a short explanation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt without context:\n",
      "('You are a classifier that, given a sentence, says if the sentence is '\n",
      " 'negative, positive, neutral or mixed. The sentence is \"And you\\'ve still got '\n",
      " 'it in your drawer, even now\". Say which class is more suitable and a short '\n",
      " 'explanation')\n",
      "Prompt with context:\n",
      "('You are a classifier that, given a sentence, says if the sentence is '\n",
      " 'negative, positive, neutral or mixed. The sentence is \"And you\\'ve still got '\n",
      " 'it in your drawer, even now\". Additional info is that: 1\". Say which class '\n",
      " 'is more suitable and a short explanation')\n"
     ]
    }
   ],
   "source": [
    "# let's check how it works\n",
    "print(\"Prompt without context:\")\n",
    "pprint.pprint(create_prompt(input_and_response[0][0], 0))\n",
    "\n",
    "print(\"Prompt with context:\")\n",
    "pprint.pprint(create_prompt(input_and_response[0][0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Llama 2 as our generative model. Since downloading it from Hugging Face didn't work due to lack of resources, we called it using the `ollama` server, that allows a user to connect with an LLM and prompt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\"model\": \"llama2\", \"prompt\": prompt}\n",
    "    full_response = []\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)\n",
    "    try:\n",
    "        for line in response.iter_lines():\n",
    "            # filter out keep-alive new lines\n",
    "            if line:\n",
    "                decoded_line = json.loads(line.decode(\"utf-8\"))\n",
    "                # print(decoded_line['response'])  # uncomment to results, token by token\n",
    "                full_response.append(decoded_line[\"response\"])\n",
    "        full_response = \"\".join(full_response)\n",
    "    finally:\n",
    "        response.close()\n",
    "        return full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call it and see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for sample_idx in range(len(input_and_response)):\n",
    "    prompt_first = create_prompt(input_and_response[sample_idx][0], None)\n",
    "    response_first = make_request(prompt_first)\n",
    "    results.append(\n",
    "        {\"sample_idx\": sample_idx, \"prompt\": prompt_first, \"response\": response_first}\n",
    "    )\n",
    "\n",
    "for sample_idx in range(len(input_and_response)):\n",
    "    prompt_both = create_prompt(\n",
    "        input_and_response[sample_idx][0], input_and_response[sample_idx][1]\n",
    "    )\n",
    "    response_both = make_request(prompt_both)\n",
    "    results.append(\n",
    "        {\"sample_idx\": sample_idx, \"prompt\": prompt_both, \"response\": response_both}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will adapt the results in a dataframe to better analyze our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df = df.sort_values(by=[\"sample_idx\"])\n",
    "df[\"lyric\"] = df[\"sample_idx\"].map(lambda index: user_inputs[index])\n",
    "df[\"label\"] = df[\"sample_idx\"].map(lambda index: labels[index])\n",
    "df[\"predicted\"] = [2, 0, 3, 3, 1, 3, 3, 0, 0, 0, 3, 0]\n",
    "df[\"RAG\"] = df[\"prompt\"].map(lambda text: \"Additional info is\" in text)\n",
    "# save it for reproducibility\n",
    "df.to_csv(\"data/output_prompt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the results of the answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = df[df.RAG == True]\n",
    "no_rag = df[df.RAG == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rag = f1_score(rag.label, rag.predicted, average=\"weighted\")\n",
    "f1_no_rag = f1_score(no_rag.label, no_rag.predicted, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_rag=0.5396825396825397, f1_no_rag=0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(f\"{f1_rag=}, {f1_no_rag=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our f1-score is higher for the non-rag prompts, hence those that were not contextualized. In this case, our additional explanations have not contributed to a better classification, however, we can affirm that adding contextual information changes the prediction and explanations of the model.\n",
    "Let's check, for each verse, how many classifications did they have with the context and without:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>nunique</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_idx</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           predicted\n",
       "             nunique\n",
       "sample_idx          \n",
       "0                  2\n",
       "1                  1\n",
       "2                  2\n",
       "3                  2\n",
       "4                  1\n",
       "5                  2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"sample_idx\", \"predicted\"]].groupby([\"sample_idx\"]).agg([\"nunique\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only two samples had the same prediction with and without the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'lyric': 'From when your Brooklyn broke my skin and bones',\n",
      " 'predicted': 3,\n",
      " 'prompt': 'You are a classifier that, given a sentence, says if the sentence '\n",
      "           'is negative, positive, neutral or mixed. The sentence is \"From '\n",
      "           'when your Brooklyn broke my skin and bones\". Say which class is '\n",
      "           'more suitable and a short explanation',\n",
      " 'response': '\\n'\n",
      "             'Based on the sentence provided, I would classify it as \"Mixed\" '\n",
      "             'because it contains both negative and positive elements.\\n'\n",
      "             '\\n'\n",
      "             'The phrase \"your Brooklyn broke my skin and bones\" is negative '\n",
      "             'in tone, as it describes physical harm caused by something '\n",
      "             '(Brooklyn) that is presumably a person or entity. The use of the '\n",
      "             'word \"broke\" implies damage or injury, which has a negative '\n",
      "             'connotation.\\n'\n",
      "             '\\n'\n",
      "             'However, the sentence also contains positive elements, such as '\n",
      "             'the use of the word \"my,\" which suggests ownership or attachment '\n",
      "             'to the thing being described. Additionally, the phrase \"skin and '\n",
      "             'bones\" could be interpreted as a metaphor for something fragile '\n",
      "             'or delicate, which has a neutral connotation.\\n'\n",
      "             '\\n'\n",
      "             'Therefore, based on the mixed nature of the sentence, I would '\n",
      "             'classify it as \"Mixed.\"',\n",
      " 'sample_idx': 5}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(df.iloc[10].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'lyric': 'From when your Brooklyn broke my skin and bones',\n",
      " 'predicted': 0,\n",
      " 'prompt': 'You are a classifier that, given a sentence, says if the sentence '\n",
      "           'is negative, positive, neutral or mixed. The sentence is \"From '\n",
      "           'when your Brooklyn broke my skin and bones\". Additional info is '\n",
      "           \"that: 'sticks and stones may break my bones' mean that it’s clear \"\n",
      "           'he put her through some sort of verbal abuse\". Say which class is '\n",
      "           'more suitable and a short explanation',\n",
      " 'response': '\\n'\n",
      "             'Based on the given sentence, I would classify it as negative. '\n",
      "             'The phrase \"From when your Brooklyn broke my skin and bones\" '\n",
      "             'suggests that someone has been physically hurt or abused, with '\n",
      "             '\"Brooklyn\" likely being a person who inflicted the harm. The '\n",
      "             'additional context you provided further reinforces this '\n",
      "             'interpretation, as \"sticks and stones may break my bones\" is '\n",
      "             'often used to convey the idea that verbal abuse can be just as '\n",
      "             'damaging as physical harm.\\n'\n",
      "             '\\n'\n",
      "             'Therefore, the most suitable class for this sentence would be '\n",
      "             '\"Negative\".',\n",
      " 'sample_idx': 5}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(df.iloc[11].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RAG system is poor and the added contextual information makes our model to predict poorly.\n",
    "However, we attribute this to the simplicity of the algorithm.\n",
    "\n",
    "Even so, we can affirm that adding contextual information can will change the behaviour of the LLM and their decisions. In good systems, we hope that the results would be more encouraging.\n",
    "\n",
    "In terms of explainability, even if our experiments didn't come as expected, prompts with additional information allow a better understanding and contextualization of a model's decision since they are understandable by a human. However, we could also argue the fact that generative models work by probability and the additional information even if it changes the given response, it might not affect positively to the correctness of the answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
